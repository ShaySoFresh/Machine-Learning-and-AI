{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Generate Sample Data***\n",
        "\n",
        "The first step is to set your OpenAI key and create a synthetic dataset representing past NFL games with various features. This dataset will include historical matchups between teams, their statistics, situational factors, and the game outcome. If your desire is to build an actual game predictor based on real data, you would skip the part that defines a random seed and synthetically generated data and simply read in your file(s), and begin cleaning and preprocessing.\n"
      ],
      "metadata": {
        "id": "c9WGzChverz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAL37iaGep0Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "import xgboost as xgb\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Define teams - just using a handful of teams for demonstation purposes\n",
        "teams = [\n",
        "    'Dallas Cowboys', 'Philadelphia Eagles', 'New England Patriots',\n",
        "    'Green Bay Packers', 'San Francisco 49ers', 'Chicago Bears',\n",
        "    'Pittsburgh Steelers', 'Denver Broncos', 'Miami Dolphins',\n",
        "    'Seattle Seahawks'\n",
        "]\n",
        "\n",
        "# Number of past games to simulate. This will give us a good amount data for\n",
        "# models to use to perform analysis on.\n",
        "num_games = 1000\n",
        "\n",
        "# Generate random data for past games\n",
        "data = {\n",
        "    'home_team': np.random.choice(teams, num_games),\n",
        "    'away_team': np.random.choice(teams, num_games),\n",
        "    'home_off_points_per_game': np.random.uniform(0, 50, num_games),\n",
        "    'home_def_points_allowed': np.random.uniform(0, 50, num_games),\n",
        "    'home_yards_per_game': np.random.uniform(100, 500, num_games),\n",
        "    'home_turnovers': np.random.uniform(0, 5, num_games),\n",
        "    'home_sacks': np.random.uniform(0, 10, num_games),\n",
        "    'away_off_points_per_game': np.random.uniform(0, 50, num_games),\n",
        "    'away_def_points_allowed': np.random.uniform(0, 50, num_games),\n",
        "    'away_yards_per_game': np.random.uniform(100, 500, num_games),\n",
        "    'away_turnovers': np.random.uniform(0, 5, num_games),\n",
        "    'away_sacks': np.random.uniform(0, 10, num_games),\n",
        "    'weather_condition': np.random.choice(['Clear', 'Rain', 'Snow', 'Windy'], num_games),\n",
        "    'day_of_week': np.random.choice(['Monday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], num_games),\n",
        "    'rest_days_home': np.random.randint(3, 14, num_games),\n",
        "    'rest_days_away': np.random.randint(3, 14, num_games),\n",
        "    'home_recent_win_streak': np.random.randint(0, 16, num_games),\n",
        "    'away_recent_win_streak': np.random.randint(0, 16, num_games),\n",
        "    'home_injuries': np.random.randint(0, 5, num_games),\n",
        "    'away_injuries': np.random.randint(0, 5, num_games),\n",
        "    'win': np.random.randint(0, 2, num_games)  # 1 if home team wins, 0 otherwise\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Let's do some cleaning and ensure home and away teams are not the same\n",
        "df = df[df['home_team'] != df['away_team']].reset_index(drop=True)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Sample of Generated Data:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explanation of What We Just Did***\n",
        "\n",
        "\t1.\tImports: We imported all the libraries we would need for the entire program including pandas for data manipulation and numpy for numerical operations.\n",
        "\t2.\tRandom Seed: Set a seed to ensure that the random data generated is reproducible.\n",
        "\t3.\tTeams: Defined a small list of teams but provided enough for diversity.\n",
        "\t4.\tNumber of Games: Set to 1000 to provide a larger dataset for better model training.\n",
        "\t5.\tData Generation: For each game, we are randomly assigned home and away teams (ensuring they’re not the same) and generated random statistics such as offensive points per game, defensive points allowed, yards per game, turnovers, sacks, weather conditions, day of the week, rest days to also account for a possible bye-week, win streaks, injuries, and the game outcome (win).\n",
        "\t6.\tDataFrame Creation: Combined the generated data into a Pandas DataFrame.\n",
        "\t7.\tData Cleaning: Removed any games where the home and away teams are the same.\n",
        "\t8.\tDisplay Data: Printed the first few rows to inspect the generated data.\n",
        "\n",
        "  Additionally, we set up our flask app to be able to interact with our OpenAI chatbot for an interactive experience that will give the end user a prompt to communicate with."
      ],
      "metadata": {
        "id": "gO19ePHwikfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPjjayM3lMmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCnQRmWXlMcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "upMuvXQxlMQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Data Preprocessing***\n",
        "\n",
        "Before training models, we need to preprocess the data.\n",
        "\n",
        "Preprocessing is critical in an NFL game predictor program for several reasons:\n",
        "\n",
        "Data Quality: Raw data often contains missing values, inconsistencies, and noise. Preprocessing helps clean and standardize this data, ensuring the quality and reliability of the predictions.\n",
        "\n",
        "Feature Engineering: Creating new features or transforming existing ones can uncover hidden patterns and relationships in the data, leading to more accurate predictions.\n",
        "\n",
        "Handling Categorical Data: Many features in NFL game data are categorical (e.g., team names, weather conditions). Preprocessing involves encoding these categorical variables into numerical formats that machine learning models can interpret.\n",
        "\n",
        "Scaling (THIS IS AN IMPORTANT ONE FOR THIS TYPE OF TOOL): Different features may have different scales, which can adversely impact the performance of some models. Scaling ensures that features are on a comparable scale, improving the performance and convergence speed of your algorithms.\n",
        "\n",
        "Reducing Dimensionality: Preprocessing can include techniques like PCA (Principal Component Analysis) to reduce the number of features, simplifying the model and speeding up training without sacrificing accuracy.\n",
        "\n",
        "Preventing Overfitting: Preprocessing techniques like data augmentation or normalization can help prevent the model from overfitting to the training data, improving its generalizability to new data.\n",
        "\n",
        "In short, preprocessing lays the groundwork for building an effective predictive tool by ensuring the data is clean, consistent, and ready for analysis. It’s like the prep work done before cooking; the better the ingredients, the tastier the dish!"
      ],
      "metadata": {
        "id": "dfi9yfEXlWc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Features and Target\n",
        "X = df.drop('win', axis=1)  # Features\n",
        "y = df['win']               # Target variable\n",
        "\n",
        "# Identify Numerical and Categorical Columns\n",
        "numerical_features = [\n",
        "    'home_off_points_per_game', 'home_def_points_allowed', 'home_yards_per_game',\n",
        "    'home_turnovers', 'home_sacks', 'away_off_points_per_game',\n",
        "    'away_def_points_allowed', 'away_yards_per_game', 'away_turnovers',\n",
        "    'away_sacks', 'rest_days_home', 'rest_days_away',\n",
        "    'home_recent_win_streak', 'away_recent_win_streak',\n",
        "    'home_injuries', 'away_injuries'\n",
        "]\n",
        "categorical_features = ['home_team', 'away_team', 'weather_condition', 'day_of_week']\n",
        "\n",
        "# Preprocessing for Numerical Data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for Categorical Data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine Preprocessing Steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n"
      ],
      "metadata": {
        "id": "_OmbgSw3fwbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explanation of What We Just Did***\n",
        "\n",
        "\t1.\tImports: Imported necessary modules from scikit-learn for preprocessing and pipeline creation.\n",
        "\t2.\tFeatures and Target:\n",
        "\t•\tX contains all features except the target variable win.\n",
        "\t•\ty is the target variable indicating whether the home team won (1) or not (0).\n",
        "\t3.\tFeature Identification:\n",
        "\t•\tNumerical Features: Continuous variables like points per game, yards, turnovers, etc.\n",
        "\t•\tCategorical Features: Discrete variables like team names, weather conditions, and days of the week.\n",
        "\t4.\tNumerical Transformer:\n",
        "\t•\tScaling: StandardScaler standardizes features by removing the mean and scaling to unit variance, which is essential for many machine learning algorithms.\n",
        "\t5.\tCategorical Transformer:\n",
        "\t•\tEncoding: OneHotEncoder converted categorical variables into a binary matrix, allowing the model to interpret them numerically.\n",
        "\t•\tHandle Unknown: handle_unknown='ignore' ensures that unseen categories during testing are ignored rather than causing errors.\n",
        "\t6.\tPreprocessor:\n",
        "\t•\tCombined both numerical and categorical transformers using ColumnTransformer, applying appropriate transformations to each type of feature."
      ],
      "metadata": {
        "id": "di8j9HnwlvkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sIFUvUE7ltNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bQmdRXNmGxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LYTyPXMllsN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Feature Engineering and Model Training***\n",
        "\n",
        "Now we’ll create pipelines for various machine learning algorithms, perform hyperparameter tuning using Grid Search, and ensemble the models to improve prediction accuracy.\n",
        "\n",
        "You might notice that the models being used are only a handful of models that could be used as there are others that will further enhance the program's process of identifying the best performing model.\n",
        "\n",
        "For this particular type of program - a game prediction tool, the following models could, and should be used to improve the program's performance:\n",
        "\n",
        "Logistic Regression: Useful for binary classification problems, it predicts the probability of a binary outcome.\n",
        "\n",
        "Linear Regression: Ideal for predicting continuous outcomes and understanding relationships between variables.\n",
        "\n",
        "Decision Trees: Easy to interpret and useful for both classification and regression tasks.\n",
        "\n",
        "Support Vector Machines (SVM): Effective for both classification and regression, especially in high-dimensional spaces.\n",
        "\n",
        "Time Series Models: Models like ARIMA (AutoRegressive Integrated Moving Average) are great for forecasting based on time series data.\n",
        "\n",
        "Elo Rating System: Often used in sports predictions, it rates teams based on their head-to-head results and can be used to generate win probabilities.\n",
        "\n",
        "Monte Carlo Simulations: Useful for simulating various game scenarios and outcomes based on probability distributions.\n",
        "\n"
      ],
      "metadata": {
        "id": "CyO9yHvhmL-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models to experiment with\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "    'NeuralNetwork': MLPClassifier(max_iter=1000, random_state=42)\n",
        "}\n",
        "\n",
        "# Define parameter grids for each model\n",
        "param_grids = {\n",
        "    'RandomForest': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [None, 10, 20],\n",
        "        'classifier__min_samples_split': [2, 5]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [3, 6],\n",
        "        'classifier__learning_rate': [0.01, 0.1]\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__learning_rate': [0.05, 0.1],\n",
        "        'classifier__max_depth': [3, 5]\n",
        "    },\n",
        "    'NeuralNetwork': {\n",
        "        'classifier__hidden_layer_sizes': [(100,), (100, 50)],\n",
        "        'classifier__activation': ['relu', 'tanh'],\n",
        "        'classifier__learning_rate_init': [0.001, 0.01]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Dictionary to store best models\n",
        "best_models = {}\n",
        "model_performance = {}\n",
        "\n",
        "# Stratified K-Fold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Iterate through each model for training and hyperparameter tuning\n",
        "for model_name in models:\n",
        "    print(f\"Training and tuning {model_name}...\")\n",
        "\n",
        "    # Create pipeline for the model\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', models[model_name])\n",
        "    ])\n",
        "\n",
        "    # Setup Grid Search\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid=param_grids[model_name],\n",
        "        cv=cv,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Fit Grid Search\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    # Store the best model\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "\n",
        "    # Evaluate using cross-validation\n",
        "    cv_scores = cross_val_score(grid_search.best_estimator_, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "\n",
        "    model_performance[model_name] = {\n",
        "        'Best Params': grid_search.best_params_,\n",
        "        'Mean ROC-AUC': cv_scores.mean(),\n",
        "        'Std ROC-AUC': cv_scores.std()\n",
        "    }\n",
        "\n",
        "    print(f\"{model_name} - Best ROC-AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ulXwhm3aoChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explanation of What We Just Did***\n",
        "\n",
        "\t1.\tImports:\n",
        "\t•\tModels: Imported various classifiers including RandomForestClassifier, XGBClassifier from XGBoost, GradientBoostingClassifier, and MLPClassifier for neural networks.\n",
        "\t•\tModel Selection: GridSearchCV for hyperparameter tuning, cross_val_score for cross-validation, and StratifiedKFold to maintain class distribution.\n",
        "\t•\tMetrics: Imported metrics like classification_report, roc_auc_score, and accuracy_score for model evaluation.\n",
        "\t•\tUtilities: joblib for saving models.\n",
        "\t2.\tModel Definitions:\n",
        "\t•\tDefined a dictionary of models containing instances of different classifiers to experiment with.\n",
        "\t3.\tParameter Grids:\n",
        "\t•\tDefined param_grids, a dictionary containing hyperparameter grids for each model. These grids specify the values to be tested during Grid Search.\n",
        "\t4.\tStorage Dictionaries:\n",
        "\t•\tbest_models: To store the best estimator for each model after hyperparameter tuning.\n",
        "\t•\tmodel_performance: To store performance metrics for each model.\n",
        "\t5.\tCross-Validation Setup:\n",
        "\t•\tUse StratifiedKFold with 5 splits to ensure that each fold maintains the class distribution, which is crucial for imbalanced datasets.\n",
        "\t6.\tModel Training and Hyperparameter Tuning:\n",
        "\t•\tIterated through each model in the models dictionary.\n",
        "\t•\tFor each model:\n",
        "\t•\tCreated a Pipeline that includes preprocessing and the classifier.\n",
        "\t•\tSetup GridSearchCV with the defined parameter grid, cross-validation strategy, ROC-AUC as the scoring metric, and parallel processing (n_jobs=-1).\n",
        "\t•\tFit GridSearchCV on the entire dataset (X, y).\n",
        "\t•\tStored the best estimator found by Grid Search in best_models.\n",
        "\t•\tEvaluated the best model using cross-validation and store the mean and standard deviation of ROC-AUC scores in model_performance.\n",
        "\t•\tPrinted out the performance metrics for each model.\n",
        "\n",
        "  Here is what sample output would look like at what it shows:\n",
        "\n",
        "Training and tuning RandomForest...\n",
        "RandomForest - Best ROC-AUC: 0.5050 ± 0.0178\n",
        "\n",
        "Training and tuning XGBoost...\n",
        "XGBoost - Best ROC-AUC: 0.5065 ± 0.0178\n",
        "\n",
        "Training and tuning GradientBoosting...\n",
        "GradientBoosting - Best ROC-AUC: 0.5083 ± 0.0182\n",
        "\n",
        "Training and tuning NeuralNetwork...\n",
        "NeuralNetwork - Best ROC-AUC: 0.5032 ± 0.0168\n",
        "\n",
        "Based on the sample output, we can see that due to the data being randomly generated, the ROC-AUC scores are all around 0.5, indicating no predictive power. In a real-world scenario with meaningful data, these scores would reflect the model’s ability to distinguish between classes and while the sample demonstrates that the GradientBoosting model performed the best, actual results might be different based on a number of factors including additional models used and how extensive your feature engineering will be.\n",
        "\n"
      ],
      "metadata": {
        "id": "MDivM7k8p49a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8sDr9VJpEek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dQTGBS6dpjlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xATJh51qtvlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ensembling: Combining Multiple Models***\n",
        "\n",
        "Ensembling can, among other things, improve prediction accuracy by combining the strengths of different models. We’ll use a Voting Classifier that aggregates the predictions of the best-performing models. Here is a more comprehensive explanation of the benefits of ensembling:\n",
        "\n",
        "Improved Accuracy: By combining the strengths of multiple models, ensembling can lead to better overall accuracy and more robust predictions.\n",
        "\n",
        "Reduced Overfitting: Individual models might overfit to the training data, but ensembling can help mitigate this by balancing out the errors.\n",
        "\n",
        "Increased Stability: The ensemble approach tends to produce more stable and reliable predictions, as it aggregates the outcomes from various models, thus reducing the impact of any single model’s errors.\n",
        "\n",
        "Enhanced Generalization: Ensembling helps models generalize better to new, unseen data, which is crucial for making reliable predictions in real-world scenarios.\n",
        "\n",
        "Versatility: Different models can capture different patterns in the data. Ensembling leverages this by combining models that might excel in various aspects, thus covering a wider range of data features and relationships.\n",
        "\n",
        "Flexibility: Ensembling allows you to incorporate various types of models (e.g., decision trees, neural networks, logistic regression) in a single framework, taking advantage of their unique strengths.\n",
        "\n",
        "In essence, ensembling is like getting a second (and third, and fourth) opinion and more often than not, it helps deliver a more accurate, reliable, and robust outcome."
      ],
      "metadata": {
        "id": "SmAhBxrJtwQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top 3 models based on ROC-AUC\n",
        "top_models = sorted(model_performance.items(), key=lambda x: x[1]['Mean ROC-AUC'], reverse=True)[:3]\n",
        "print(\"\\nTop Models for Ensembling:\")\n",
        "for model in top_models:\n",
        "    print(f\"{model[0]} - Mean ROC-AUC: {model[1]['Mean ROC-AUC']:.4f} ± {model[1]['Std ROC-AUC']:.4f}\")\n",
        "\n",
        "# Extract the best estimators\n",
        "estimators = [(name, best_models[name]['classifier']) for name, _ in top_models]\n",
        "\n",
        "# Create Voting Classifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=estimators,\n",
        "    voting='soft'  # 'soft' uses predicted probabilities\n",
        ")\n",
        "\n",
        "# Create Pipeline with Voting Classifier\n",
        "voting_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', voting_clf)\n",
        "])\n",
        "\n",
        "# Perform Cross-Validation\n",
        "voting_cv_scores = cross_val_score(voting_pipeline, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "print(f\"\\nEnsembled Voting Classifier - Mean ROC-AUC: {voting_cv_scores.mean():.4f} ± {voting_cv_scores.std():.4f}\")\n",
        "\n",
        "# Fit the Voting Classifier on the entire dataset\n",
        "voting_pipeline.fit(X, y)\n",
        "\n",
        "# Store the ensembled model\n",
        "best_models['VotingClassifier'] = voting_pipeline\n",
        "model_performance['VotingClassifier'] = {\n",
        "    'Mean ROC-AUC': voting_cv_scores.mean(),\n",
        "    'Std ROC-AUC': voting_cv_scores.std()\n",
        "}\n"
      ],
      "metadata": {
        "id": "vkmUzlEfpjXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explanation of What We Just Did***\n",
        "\n",
        "\n",
        "\t1.\tSelect Top Models:\n",
        "\t•\tSorted the model_performance dictionary based on the mean ROC-AUC scores.\n",
        "\t•\tSelected the top 3 models to include in the ensemble.\n",
        "\t2.\tExtract Best Estimators:\n",
        "\t•\tFor each of the top models, extracted the best classifier found during Grid Search.\n",
        "\t3.\tVoting Classifier:\n",
        "\t•\tCreated a VotingClassifier that combines the predictions of the selected models.\n",
        "\t•\tUsed 'soft' voting, which averages the predicted probabilities, providing a more nuanced aggregation.\n",
        "\t4.\tPipeline with Voting Classifier:\n",
        "\t•\tCreated a Pipeline that includes preprocessing and the VotingClassifier.\n",
        "\t5.\tCross-Validation:\n",
        "\t•\tEvaluated the ensemble using cross-validation to obtain the mean and standard deviation of ROC-AUC scores.\n",
        "\t6.\tFit and Store:\n",
        "\t•\tFit the VotingClassifier on the entire dataset.\n",
        "\t•\tStored the ensembled model and its performance metrics.\n",
        "  "
      ],
      "metadata": {
        "id": "uheWcfElu7U3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHsEQJ3Wuqif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nlKqmrzuqLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zwp-6QgquqBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Validation and Testing***\n",
        "\n",
        "We’ll implement k-Fold Cross-Validation to ensure the model generalizes well to unseen data. This step has already been incorporated in the previous sections, but we’ll elaborate further.\n"
      ],
      "metadata": {
        "id": "if75EO3Lze_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed Cross-Validation for the Best Individual Model\n",
        "best_individual_model = top_models[0][0]  # Assuming the first in top_models is the best\n",
        "print(f\"\\nDetailed Cross-Validation for {best_individual_model}:\")\n",
        "\n",
        "# Retrieve the best estimator\n",
        "best_estimator = best_models[best_individual_model]\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(best_estimator, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "\n",
        "print(f\"{best_individual_model} - Mean ROC-AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "# Detailed Cross-Validation for Voting Classifier\n",
        "print(\"\\nDetailed Cross-Validation for Voting Classifier:\")\n",
        "\n",
        "# Perform cross-validation\n",
        "voting_cv_scores = cross_val_score(voting_pipeline, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "\n",
        "print(f\"VotingClassifier - Mean ROC-AUC: {voting_cv_scores.mean():.4f} ± {voting_cv_scores.std():.4f}\")\n"
      ],
      "metadata": {
        "id": "NnouvxqzzdeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explanation of What We Just Did***\n",
        "\n",
        "\t1.\tBest Individual Model:\n",
        "\t•\tIdentified the best individual model from the top models selected earlier.\n",
        "\t2.\tRetrieve Best Estimator:\n",
        "\t•\tExtracted the best estimator corresponding to the selected model.\n",
        "\t3.\tCross-Validation:\n",
        "\t•\tPerformed cross-validation using cross_val_score with ROC-AUC as the scoring metric.\n",
        "\t•\tPrinted out the mean and standard deviation of ROC-AUC scores to assess model performance.\n",
        "\t4.\tVoting Classifier:\n",
        "\t•\tSimilarly, performed cross-validation for the ensembled VotingClassifier.\n",
        "\t•\tPrinted out its performance metrics.\n",
        "\n",
        "Sample Output:\n",
        "\n",
        "Detailed Cross-Validation for GradientBoosting:\n",
        "GradientBoosting - Mean ROC-AUC: 0.5083 ± 0.0182\n",
        "\n",
        "Detailed Cross-Validation for Voting Classifier:\n",
        "VotingClassifier - Mean ROC-AUC: 0.5090 ± 0.0185\n",
        "\n",
        "Again, scores are around 0.5 due to random data.\n"
      ],
      "metadata": {
        "id": "g59r0FJD0jox"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fx1j3i60h1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYhuEo5X0hr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3sU6cgtj0het"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NOW THE FUN BEGINS!!!!***\n",
        "\n",
        "This is the phase where we will now implement our synthetically produced, cleaned, preprocessed, trained and predictive analyzed data into the best part: the interactive communication channel with our chatbot by integrating OpenAI's GPT-3 for generating natural language content in conjunction with the predicted insights and analysis with creative and insightful commentary!\n",
        "\n",
        "Here's a breakdown of what is about to take place under the hood:\n",
        "\n",
        "\n",
        "Step-by-Step Approach:\n",
        "\n",
        "Prediction from the Model: Use the trained model to get the prediction and probabilities.\n",
        "\n",
        "Generative Content with GPT-3: Use GPT-3 to generate original content based on the model's predictions, player stats, game analysis, and even possible game simulations.\n",
        "\n",
        "Combining Both: Combine the model’s prediction with the generative content to deliver a comprehensive and engaging response."
      ],
      "metadata": {
        "id": "ddpznv5L1R4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load pre-trained model\n",
        "voting_pipeline = joblib.load(\"voting_pipeline.pkl\")  # Assuming you've saved your model\n",
        "\n",
        "# Example route to receive input and return predictions\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json()\n",
        "\n",
        "    # Extract data from request\n",
        "    home_team = data.get('home_team')\n",
        "    away_team = data.get('away_team')\n",
        "    home_off_points_per_game = data.get('home_off_points_per_game')\n",
        "    home_def_points_allowed = data.get('home_def_points_allowed')\n",
        "    home_yards_per_game = data.get('home_yards_per_game')\n",
        "    home_turnovers = data.get('home_turnovers')\n",
        "    home_sacks = data.get('home_sacks')\n",
        "    away_off_points_per_game = data.get('away_off_points_per_game')\n",
        "    away_def_points_allowed = data.get('away_def_points_allowed')\n",
        "    away_yards_per_game = data.get('away_yards_per_game')\n",
        "    away_turnovers = data.get('away_turnovers')\n",
        "    away_sacks = data.get('away_sacks')\n",
        "    weather_condition = data.get('weather_condition')\n",
        "    day_of_week = data.get('day_of_week')\n",
        "    rest_days_home = data.get('rest_days_home')\n",
        "    rest_days_away = data.get('rest_days_away')\n",
        "    home_recent_win_streak = data.get('home_recent_win_streak')\n",
        "    away_recent_win_streak = data.get('away_recent_win_streak')\n",
        "    home_injuries = data.get('home_injuries')\n",
        "    away_injuries = data.get('away_injuries')\n",
        "\n",
        "    # Create DataFrame for the input data\n",
        "    input_data = pd.DataFrame({\n",
        "        'home_team': [home_team],\n",
        "        'away_team': [away_team],\n",
        "        'home_off_points_per_game': [home_off_points_per_game],\n",
        "        'home_def_points_allowed': [home_def_points_allowed],\n",
        "        'home_yards_per_game': [home_yards_per_game],\n",
        "        'home_turnovers': [home_turnovers],\n",
        "        'home_sacks': [home_sacks],\n",
        "        'away_off_points_per_game': [away_off_points_per_game],\n",
        "        'away_def_points_allowed': [away_def_points_allowed],\n",
        "        'away_yards_per_game': [away_yards_per_game],\n",
        "        'away_turnovers': [away_turnovers],\n",
        "        'away_sacks': [away_sacks],\n",
        "        'weather_condition': [weather_condition],\n",
        "        'day_of_week': [day_of_week],\n",
        "        'rest_days_home': [rest_days_home],\n",
        "        'rest_days_away': [rest_days_away],\n",
        "        'home_recent_win_streak': [home_recent_win_streak],\n",
        "        'away_recent_win_streak': [away_recent_win_streak],\n",
        "        'home_injuries': [home_injuries],\n",
        "        'away_injuries': [away_injuries]\n",
        "    })\n",
        "\n",
        "    # Make prediction using the Voting Classifier\n",
        "    pred = voting_pipeline.predict(input_data)\n",
        "    proba = voting_pipeline.predict_proba(input_data)[:, 1]\n",
        "\n",
        "    # Interpret the prediction\n",
        "    if pred[0] == 1:\n",
        "        outcome = f\"{home_team} are predicted to WIN.\"\n",
        "    else:\n",
        "        outcome = f\"{away_team} are predicted to WIN.\"\n",
        "\n",
        "    f\"{away_team} are predicted to WIN.\"\n",
        "\n",
        "    # Use GPT-3 to generate in-depth analysis and game summary\n",
        "    gpt3_prompt = (\n",
        "        f\"The upcoming game between {home_team} and {away_team} is generating a lot of buzz. Based on the current data, \"\n",
        "        f\"the prediction is that {prediction_result}.\\n\\n\"\n",
        "        f\"Here are the key stats:\\n\"\n",
        "        f\"- {home_team} Offense Points Per Game: {home_off_points_per_game}\\n\"\n",
        "        f\"- {home_team} Defense Points Allowed: {home_def_points_allowed}\\n\"\n",
        "        f\"- {away_team} Offense Points Per Game: {away_off_points_per_game}\\n\"\n",
        "        f\"- {away_team} Defense Points Allowed: {away_def_points_allowed}\\n\"\n",
        "        \"Can you provide a detailed analysis and game summary including potential key plays, strategies, and player performances?\"\n",
        "    )\n",
        "\n",
        "    gpt3_response = openai.Completion.create(\n",
        "        engine=\"davinci-codex\",\n",
        "        prompt=gpt3_prompt,\n",
        "        max_tokens=200\n",
        "    )\n",
        "\n",
        "    detailed_analysis = gpt3_response['choices'][0]['text'].strip()\n",
        "\n",
        "    response = {\n",
        "        \"prediction\": prediction_result,\n",
        "        \"probability\": float(proba[0]),\n",
        "        \"detailed_analysis\": detailed_analysis\n",
        "    }\n",
        "\n",
        "    return jsonify(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "mYcxLDYdupyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Saving the Model###\n",
        "#Make sure you save your trained model so you can load it in your Flask app:\n",
        "\n",
        "# Save the trained voting pipeline model\n",
        "joblib.dump(voting_pipeline, 'voting_pipeline.pkl')\n",
        "\n",
        "###Running the Flask App###\n",
        "\n",
        "#Save your Flask app code in a file, say nfl_predict.py.\n",
        "\n",
        "#Run your Flask app from the command line:\n",
        "#python nfl_predict.py\n",
        "\n",
        "###Sending POST Requests:###\n",
        "\n",
        "#You interact with the chatbot by sending POST requests to the\n",
        "#endpoint http://127.0.0.1:5000/predict.\n",
        "\n",
        "#These requests should contain the game data in JSON format.\n",
        "\n",
        "###Preparing the JSON Payload:###\n",
        "\n",
        "#The JSON payload should include all the necessary game-related data that the\n",
        "#model needs to make a prediction. Here’s a template for what your JSON payload\n",
        "#might look like:\n",
        "\n",
        "#{\n",
        "#    \"home_team\": \"Dallas Cowboys\",\n",
        "#    \"away_team\": \"Philadelphia Eagles\",\n",
        "#    \"home_off_points_per_game\": 28.5,\n",
        "#    \"home_def_points_allowed\": 22.0,\n",
        "#    \"home_yards_per_game\": 420,\n",
        "#    \"home_turnovers\": 8,\n",
        "#    \"home_sacks\": 6,\n",
        "#    \"away_off_points_per_game\": 26.0,\n",
        "#    \"away_def_points_allowed\": 24.0,\n",
        "#    \"away_yards_per_game\": 410,\n",
        "#    \"away_turnovers\": 7,\n",
        "#    \"away_sacks\": 5,\n",
        "#    \"weather_condition\": \"Clear\",\n",
        "#    \"day_of_week\": \"Sunday\",\n",
        "#    \"rest_days_home\": 5,\n",
        "#    \"rest_days_away\": 4,\n",
        "#    \"home_recent_win_streak\": 2,\n",
        "#    \"away_recent_win_streak\": 1,\n",
        "#    \"home_injuries\": 1,\n",
        "#    \"away_injuries\": 2\n",
        "#}\n",
        "\n",
        "\n",
        "###Interpreting the Response:###\n",
        "\n",
        "#After you send the POST request, the Flask server processes the data and returns a response.\n",
        "\n",
        "#The response will be in JSON format and will include:\n",
        "\n",
        "#Prediction: Which team is predicted to win.\n",
        "\n",
        "#Probability: The confidence level of the prediction.\n",
        "\n",
        "#Detailed Analysis: A generated summary and analysis of the game, including key player stats, potential strategies, and more.\n",
        "\n"
      ],
      "metadata": {
        "id": "9WH3X2wZ5oMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on successfully harnessing the power of machine learning, predictive analysis, and combining it with generative AI, to create an NFL prediction tool! 🎉 Your journey into these advanced technologies has not only enabled you to build an exciting, interactive application but also laid a solid foundation for a future filled with endless possibilities.\n",
        "\n",
        "By mastering these skills, you can gain the ability to transform raw data into meaningful insights and engage users in ways that were once the realm of science fiction and apply these methodologies into other fields such as healthcare or finance to name a few.\n",
        "\n",
        "Imagine what else you can achieve with this knowledge: from personalized recommendation systems and intelligent chatbots to predictive maintenance tools and sophisticated financial models. The world of machine learning and AI is vast and brimming with opportunities for those who dare to explore it.\n",
        "\n",
        "So, keep pushing the boundaries, stay curious, and continue creating tools that inspire and make a difference. The future is yours to shape, one innovative project at a time. 🚀\n",
        "\n",
        "Well done, and here's to many more groundbreaking endeavors! 👏✨"
      ],
      "metadata": {
        "id": "N8tNoUMU8j0I"
      }
    }
  ]
}